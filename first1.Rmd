---
title: "try1"
author: "Sourav Jose"
date: "2025-03-26"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(repos = c(CRAN = "https://cloud.r-project.org"))

```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r}

```

## Including Plots

```{r}
#loading the data and libraries 
library(learningtower)

library(patchwork)
# Load libraries
library(tidyverse)
library(ggplot2)
library(cluster)       # For clustering
    # For visualizing clusters
       # For machine learning
library(randomForest)   # For Random Forest
library(e1071)          # For SVM
      # For XGBoost
library(plotly)         # For interactive plots

install.packages("xgboost")
library(xgboost)  
install.packages("caret")
library(caret)   
install.packages("factoextra")
library(factoextra) 
install.packages("gghighlight")
library(gghighlight)
install.packages("ggrepel")
library(ggrepel)
install.packages("tsibble")
library(tsibble)
install.packages("kableExtra")
library(kableExtra)
library(broom)
student <- load_student("all")
data(countrycode)

theme_set(theme_classic(18) +
            theme(legend.position = "bottom"))
```

```{r}
ireland_data <- student %>% filter(country == "IRL")
ireland_data

```



```{r}
# Checking the structure of the ireland_data
str(ireland_data)

# Building a linear regression model for math scores based on the available factors
model <- lm(math ~ gender + room + television + computer + book + wealth + escs, 
            data = ireland_data)

# Summary of the model
summary(model)


```


```{r}
library(ggplot2)

# Assuming you have math scores and year in your dataset
ggplot(ireland_data, aes(x = year, y = math)) +
  geom_line() +
  ggtitle("PISA Math Scores Trend (2000-2022)") +
  xlab("Year") +
  ylab("Average Math Score")

```
The math scores appear to fluctuate each year, but there isn't a clear upward or downward trend. Instead, the scores show moderate variability.

There are periods where the scores increase slightly and others where they decrease, but the average appears relatively stable over the entire period.

```{r}
# Scatter plot with regression line
plot(ireland_data$escs, ireland_data$math,
     main = "Scatter Plot of ESCS and Math Scores with Regression Line",
     xlab = "ESCS", ylab = "Math Score",
     pch = 19, col = rgb(0, 0, 1, 0.5))
abline(lm(math ~ escs, data = ireland_data), col = "red", lwd = 2)  # Add red regression line



```
The plot shows that socioeconomic status (as measured by ESCS) is an important factor influencing math performance. Students from more affluent backgrounds tend to score higher on math assessments


```{r}
ireland_data %>%
  group_by(year, gender) %>%
  summarise(mean_math = mean(math, na.rm = TRUE)) %>%
  ggplot(aes(x = year, y = mean_math, color = gender)) +
  geom_line() + geom_point() +
  labs(title = "Math Score Trends by Gender", y = "Mean Math Score", x = "Year")

```

 The graph clearly shows that, in most years, male students consistently outperform female students in PISA math assessments. This difference in average scores is observable at every time point, with male students typically showing higher average scores than their female counterparts.

```{r}
# Calculate the average math score by number of books
avg_math_books <- aggregate(math ~ book, data = ireland_data, FUN = mean)

# Bar plot for average math scores by number of books
barplot(avg_math_books$math, names.arg = avg_math_books$book,
        col = "lightblue", main = "Average Math Scores by Number of Books",
        xlab = "Number of Books", ylab = "Average Math Score")




```


The plot suggests that having access to books does not drastically change average math scores, but there might be a small advantage for students with a moderate number of books. The slight drop for the "none" category suggests that having no books could be a disadvantage, although the difference is subtle.


```{r}

# Bar plot for math scores by television access
aggregate_math_tv <- aggregate(math ~ television, data = ireland_data, FUN = mean)
barplot(aggregate_math_tv$math, names.arg = aggregate_math_tv$television,
        col = "lightyellow", main = "Average Math Scores by Television Access",
        xlab = "Television Access", ylab = "Average Math Score")



```

All four bars have a similar height, indicating that there is no significant difference in the average math scores between students who have varying numbers of televisions in their households.

The data seems to show that the number of televisions does not appear to have a strong relationship with the average math score in this dataset.


```{r}


# Histogram for math scores by computer access
hist(ireland_data$math[ireland_data$computer_n == 1], col = "blue", xlim = c(0, 800),
     main = "Distribution of Math Scores with Computer Access", xlab = "Math Scores", 
     ylab = "Frequency", breaks = 20)

hist(ireland_data$math[ireland_data$computer_n == 0], col = "red", add = TRUE, 
     breaks = 20)
legend("topright", legend = c("With Computer", "Without Computer"), fill = c("blue", "red"))


```
The histogram suggests that students with computer access tend to have a more evenly distributed performance across various math score ranges, with some achieving high scores.

On the other hand, students without computer access are more concentrated in the lower to mid-range math scores, indicating that lack of computer access might be associated with lower math performance.

```{r}

# Count of students with and without internet access
internet_count <- table(ireland_data$internet)

# Pie chart of internet access distribution
pie(internet_count, labels = names(internet_count), col = c("lightgreen", "lightblue"), 
    main = "Distribution of Internet Access")



```


```{r}

ireland_data %>%
  group_by(year, internet) %>%
  summarise(mean_math = mean(math, na.rm = TRUE)) %>%
  ggplot(aes(x = year, y = mean_math, color = internet)) +
  geom_line() + geom_point() +
  labs(title = "Math Scores by Internet Access Over Years", y = "Mean Math Score", x = "Year")

# Count of students with and without internet access
internet_count <- table(ireland_data$internet)

# Pie chart of internet access distribution
pie(internet_count, labels = names(internet_count), col = c("lightgreen", "lightblue"), 
    main = "Distribution of Internet Access")

```
The plot shows that internet access does not seem to drastically improve math scores over time. However, students with internet access tend to score slightly higher than those without internet access.


```{r}





```

```{r}


```

```{r}
# Calculate average math scores by gender
avg_math_by_gender <- aggregate(math ~ gender, data = ireland_data, FUN = mean)

# Bar plot of average math scores by gender
barplot(avg_math_by_gender$math, names.arg = avg_math_by_gender$gender, 
        col = c("pink", "lightblue"), main = "Average Math Scores by Gender", 
        xlab = "Gender", ylab = "Average Math Score")

```

```{r}

```

```{r}


```





```{r}


# Boxplot for father's education vs. math scores
boxplot(math ~ father_educ, data = ireland_data, 
        main = "Math Scores by Father's Education Level",
        xlab = "Father's Education Level", ylab = "Math Score",
        col = "lightcoral", border = "darkred")

```
The plot suggests that students whose fathers have higher education levels tend to have higher median math scores. The difference in medians between ISCED 1 and ISCED 3A, for example, is noticeable.

The presence of outliers in all groups suggests that while the general trend is that higher paternal education is associated with better math performance, there are exceptions.
```{r}
# Boxplot for mother's education vs. math scores
boxplot(math ~ mother_educ, data = ireland_data, 
        main = "Math Scores by Mother's Education Level",
        xlab = "Mother's Education Level", ylab = "Math Score",
        col = "lightblue", border = "darkblue")
```

The plot suggests that students whose mothers have higher education levels tend to have higher math scores. The median score for students with mothers who have ISCED 3A or ISCED 3B, C is significantly higher than for those with mothers who have ISCED 1 or ISCED 2.

The presence of outliers in all groups suggests that while the general trend is that higher maternal education is associated with better math performance, there are exceptions.


```{r}

# Average math score over years
avg_math_year <- ireland_data %>%
  group_by(year) %>%
  summarise(mean_math = mean(math, na.rm = TRUE))

# Plot
ggplot(avg_math_year, aes(x = year, y = mean_math)) +
  geom_line() +
  geom_point() +
  theme_minimal() +
  labs(title = "Trend of Math Scores Over Years in Ireland", y = "Average Math Score")

```
```{r}


library(dplyr)
library(ggplot2)
library(factoextra)
library(cluster)

# Step 1: Filter and select only the relevant features (math and read)
ireland_2018 <- ireland_data %>%
  filter(year == 2018) %>%
  dplyr::select(math, read) %>%
  drop_na()  # Remove rows with missing values

# Step 2: Scale the data (important for K-means clustering)
scaled_cluster_data_2018 <- scale(ireland_2018)

# Step 3: Elbow Method to find the optimal number of clusters
wss <- sapply(1:10, function(k) {
  kmeans(scaled_cluster_data_2018, centers = k, nstart = 25)$tot.withinss
})

# Step 4: Plot the Elbow Method
elbow_plot <- ggplot(data.frame(k = 1:10, wss = wss), aes(x = k, y = wss)) +
  geom_line() +
  geom_point() +
  theme_minimal() +
  labs(title = "Elbow Method for Optimal Clusters",
       x = "Number of Clusters",
       y = "Within-cluster Sum of Squares (WSS)")

print(elbow_plot)

# Step 5: Silhouette Method to assess clustering quality
silhouette_scores <- sapply(2:10, function(k) {
  kmeans_model <- kmeans(scaled_cluster_data_2018, centers = k, nstart = 25)
  silhouette_score <- silhouette(kmeans_model$cluster, dist(scaled_cluster_data_2018))
  mean(silhouette_score[, 3])  # Average silhouette width
})

# Step 6: Plot Silhouette Method
silhouette_plot <- ggplot(data.frame(k = 2:10, silhouette_score = silhouette_scores), aes(x = k, y = silhouette_score)) +
  geom_line() +
  geom_point() +
  theme_minimal() +
  labs(title = "Silhouette Method for Optimal Clusters",
       x = "Number of Clusters",
       y = "Average Silhouette Score")

print(silhouette_plot)

```


```{r}
library(dplyr)
library(ggplot2)
library(factoextra)

# Step 1: Filter and select only the relevant features (read and math)
ireland_2018 <- ireland_data %>%
  filter(year == 2018) %>%
  dplyr::select(math, read) %>%
  drop_na()  # Remove rows with missing values

# Step 2: Prepare clustering data with only read and math scores
cluster_data_2018 <- ireland_2018 %>%
  dplyr::select(math, read)

# Step 3: Scale the data to standardize it
scaled_cluster_data_2018 <- scale(cluster_data_2018)

# Step 4: Run K-means clustering (let's assume 2 clusters for simplicity)
set.seed(123)  # Set seed for reproducibility
kmeans_2018 <- kmeans(scaled_cluster_data_2018, centers = 2, nstart = 25)

# Step 5: Attach cluster labels to the original data
ireland_2018$cluster <- factor(kmeans_2018$cluster)

# Step 6: Analyze the clusters (mean math and read scores per cluster)
cluster_summary <- ireland_2018 %>%
  group_by(cluster) %>%
  summarise(mean_math = mean(math), mean_read = mean(read), n = n())

print(cluster_summary)

# Step 7: Scatter plot of read vs math scores by cluster
ggplot(ireland_2018, aes(x = read, y = math, color = cluster)) +
  geom_point(alpha = 0.7, size = 3) +
  theme_minimal() +
  labs(title = "Scatter Plot of Read vs Math Scores  (2018)",
       x = "Read Score", y = "Math Score") +
  scale_color_manual(values = c("blue", "red"))

```

```{r}

# Load necessary libraries
library(dplyr)
library(ggplot2)
library(randomForest)

# Step 1: Filter and select only relevant numeric features
ireland_2018 <- ireland_data %>%
  filter(year == 2018) %>%
  dplyr::select(math, wealth, escs, read, science, internet) %>%
  drop_na()  # Drop rows with missing values

# Step 2: Split the data into training and testing sets (80% training, 20% testing)
set.seed(123)  # For reproducibility
train_indices <- sample(1:nrow(ireland_2018), size = 0.8 * nrow(ireland_2018))

train_data <- ireland_2018[train_indices, ]
test_data <- ireland_2018[-train_indices, ]

# Step 3: Train a Random Forest model
rf_model <- randomForest(math ~ wealth + escs + read + science + internet,
                         data = train_data,
                         ntree = 500,      # Number of trees
                         mtry = 2,         # Number of variables randomly sampled at each split
                         importance = TRUE # Calculate feature importance
)

# Print model details
print(rf_model)

# Step 4: Make predictions on the test set
predictions <- predict(rf_model, test_data)

# Step 5: Evaluate the model using Mean Squared Error (MSE)
mse <- mean((predictions - test_data$math)^2)
cat("Mean Squared Error (MSE):", mse, "\n")

# Step 6: Calculate R-squared to evaluate how well the model fits the data
rsq <- 1 - sum((predictions - test_data$math)^2) / sum((mean(test_data$math) - test_data$math)^2)
cat("R-squared:", rsq, "\n")

# Step 7: Display feature importance
importance(rf_model)
varImpPlot(rf_model)

# Step 8: Visualize predictions vs actual values (optional)
ggplot(data.frame(actual = test_data$math, predicted = predictions), aes(x = actual, y = predicted)) +
  geom_point() +
  geom_smooth(method = "lm", color = "red") +
  theme_minimal() +
  labs(title = "Actual vs Predicted Math Scores", x = "Actual Math Scores", y = "Predicted Math Scores")




```

```{r}


```

```{r}

# Load the necessary libraries
library(dplyr)
library(randomForest)
library(ggplot2)

# Step 1: Filter the dataset to include only math and read scores
ireland_2018 <- ireland_data %>%
  filter(year == 2018) %>%
  dplyr::select(math, read) %>%
  drop_na()  # Remove rows with missing values

# Step 2: Split the data into training and test sets (80% training, 20% test)
set.seed(123)  # For reproducibility
train_indices <- sample(1:nrow(ireland_2018), size = 0.8 * nrow(ireland_2018))

train_data <- ireland_2018[train_indices, ]
test_data <- ireland_2018[-train_indices, ]

# Step 3: Train a Random Forest model
rf_model <- randomForest(math ~ read,
                         data = train_data,
                         ntree = 500,      # Number of trees
                         mtry = 1,         # Number of variables to sample at each split
                         importance = TRUE # Calculate feature importance
)

# Print model summary
print(rf_model)

# Step 4: Predict on the test set
predictions <- predict(rf_model, test_data)

# Step 5: Evaluate the model performance using Mean Squared Error (MSE)
mse <- mean((predictions - test_data$math)^2)
cat("Mean Squared Error (MSE):", mse, "\n")

# Step 6: Calculate R-squared to assess how well the model fits the data
rsq <- 1 - sum((predictions - test_data$math)^2) / sum((mean(test_data$math) - test_data$math)^2)
cat("R-squared:", rsq, "\n")

# Step 7: Display feature importance (in this case, it's only the 'read' feature)
importance(rf_model)

# Step 8: Visualize the predicted vs actual math scores
ggplot(data.frame(actual = test_data$math, predicted = predictions), aes(x = actual, y = predicted)) +
  geom_point() +
  geom_smooth(method = "lm", color = "red") +
  theme_minimal() +
  labs(title = "Actual vs Predicted Math Scores", x = "Actual Math Scores", y = "Predicted Math Scores")


```
